# ========== IMPORTS ==========
import os
import glob
import json
from dotenv import load_dotenv
import gradio as gr
from langchain.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from openai import OpenAI
import numpy as np
from sklearn.manifold import TSNE
import plotly.graph_objects as go

# ========== CONFIGURATION ==========
load_dotenv(override=True)

# Use FREE embeddings instead of OpenAI
MODEL = "gpt-3.5-turbo"  # Changed to reliable model
db_name = "vector_db"

# ========== PHASE 1: BUILD KNOWLEDGE BASE ==========
print("üîÑ Step 1: Loading and processing documents...")

# Read in documents using LangChain's loaders
folders = glob.glob("knowledge-base/*")
text_loader_kwargs = {'encoding': 'utf-8'}

documents = []
for folder in folders:
    doc_type = os.path.basename(folder)
    loader = DirectoryLoader(folder, glob="**/*.md", loader_cls=TextLoader, loader_kwargs=text_loader_kwargs)
    folder_docs = loader.load()
    for doc in folder_docs:
        doc.metadata["doc_type"] = doc_type
        documents.append(doc)

print(f"‚úÖ Loaded {len(documents)} documents")

# Split documents into chunks
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = text_splitter.split_documents(documents)

doc_types = set(chunk.metadata['doc_type'] for chunk in chunks)
print(f"üìÑ Document types found: {', '.join(doc_types)}")
print(f"‚úÖ Total chunks created: {len(chunks)}")

# ========== SETUP EMBEDDINGS ==========
print("üîÑ Setting up embeddings...")

try:
    # HuggingFace Embeddings
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        model_kwargs={'device': 'cpu'}
    )
    
    # Test embedding
    test_embedding = embeddings.embed_query("Test message")
    print(f"‚úÖ HuggingFace Embeddings working! Dimension: {len(test_embedding)}")

except Exception as e:
    print(f"‚ùå Error with HuggingFace embeddings: {e}")
    print("üîÑ Trying OpenAI embeddings as fallback...")
    
    try:
        from langchain_openai import OpenAIEmbeddings
        embeddings = OpenAIEmbeddings()
        print("‚úÖ Using OpenAI embeddings as fallback")
    except Exception as e2:
        print(f"‚ùå No embedding method available: {e2}")
        raise Exception("No working embeddings found")

# ========== CREATE VECTOR DATABASE ==========
print("üîÑ Creating vector database...")

vectorstore = Chroma.from_documents(
    documents=chunks, 
    embedding=embeddings, 
    persist_directory=db_name
)

print(f"‚úÖ Vectorstore created with {vectorstore._collection.count()} documents")

# ========== VISUALIZATION (Optional) ==========
print("üîÑ Creating visualizations...")

try:
    collection = vectorstore._collection
    result = collection.get(include=['embeddings', 'documents', 'metadatas'])
    vectors = np.array(result['embeddings'])
    documents_text = result['documents']
    doc_types = [metadata['doc_type'] for metadata in result['metadatas']]
    
    # Create color mapping
    unique_doc_types = list(set(doc_types))
    color_map = {doc_type: f'rgb({hash(doc_type) % 255}, {hash(doc_type + "1") % 255}, {hash(doc_type + "2") % 255})' 
                 for doc_type in unique_doc_types}
    colors = [color_map[t] for t in doc_types]

    # 2D Visualization
    tsne_2d = TSNE(n_components=2, random_state=42)
    reduced_vectors_2d = tsne_2d.fit_transform(vectors)

    fig_2d = go.Figure(data=[go.Scatter(
        x=reduced_vectors_2d[:, 0],
        y=reduced_vectors_2d[:, 1],
        mode='markers',
        marker=dict(size=5, color=colors, opacity=0.8),
        text=[f"Type: {t}<br>Text: {d[:100]}..." for t, d in zip(doc_types, documents_text)],
        hoverinfo='text'
    )])

    fig_2d.update_layout(
        title='2D Document Embeddings Visualization',
        xaxis_title='Dimension 1',
        yaxis_title='Dimension 2',
        width=800,
        height=600
    )
    fig_2d.show()

    # 3D Visualization
    tsne_3d = TSNE(n_components=3, random_state=42)
    reduced_vectors_3d = tsne_3d.fit_transform(vectors)

    fig_3d = go.Figure(data=[go.Scatter3d(
        x=reduced_vectors_3d[:, 0],
        y=reduced_vectors_3d[:, 1],
        z=reduced_vectors_3d[:, 2],
        mode='markers',
        marker=dict(size=5, color=colors, opacity=0.8),
        text=[f"Type: {t}<br>Text: {d[:100]}..." for t, d in zip(doc_types, documents_text)],
        hoverinfo='text'
    )])

    fig_3d.update_layout(
        title='3D Document Embeddings Visualization',
        scene=dict(xaxis_title='X', yaxis_title='Y', zaxis_title='Z'),
        width=900,
        height=700
    )
    fig_3d.show()
    
    print("‚úÖ Visualizations created successfully!")

except Exception as e:
    print(f"‚ö†Ô∏è Visualization failed: {e}")
    print("Continuing with chatbot setup...")

# ========== SETUP CONVERSATIONAL CHAIN ==========
print("üîÑ Setting up conversational chain with memory...")

# Create LLM
llm = ChatOpenAI(temperature=0.7, model_name=MODEL)

# Set up the conversation memory for the chat
memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

# The retriever is an abstraction over the VectorStore that will be used during RAG
retriever = vectorstore.as_retriever()

# Putting it together: set up the conversation chain with the LLM, the vector store and memory
conversation_chain = ConversationalRetrievalChain.from_llm(
    llm=llm, 
    retriever=retriever, 
    memory=memory,
    return_source_documents=True
)

print("‚úÖ Conversational chain with memory created!")

# ========== CHAT SYSTEM SETUP ==========
print("üîÑ Setting up chat system...")

# Initialize OpenAI client for tool functionality
openai_client = OpenAI()

# System prompt for tool-based chat
system_message = (
    "You are a helpful and professional assistant for Intensity, a company providing "
    "cybersecurity, cloud, and AI solutions. "
    "Answer customer questions clearly and accurately. "
    "Use the provided context from company documents to answer questions. "
    "Keep answers concise, but you may provide more details if the user seems to need them. "
    "Maintain a courteous and friendly tone. "
    "If you don't know the answer based on the context, politely admit it and suggest contacting an Intensity expert for further help."
)

# Debug API keys
openai_api_key = os.getenv("OPENAI_API_KEY")
anthropic_api_key = os.getenv("ANTHROPIC_API_KEY")
google_api_key = os.getenv("GOOGLE_API_KEY")

if openai_api_key:
    print(f"‚úÖ OpenAI API Key exists and begins {openai_api_key[:8]}...")
else:
    print("‚ùå OpenAI API Key not set")

if anthropic_api_key:
    print(f"‚úÖ Anthropic API Key exists and begins {anthropic_api_key[:7]}...")
else:
    print("‚ùå Anthropic API Key not set")

if google_api_key:
    print(f"‚úÖ Google API Key exists and begins {google_api_key[:8]}...")
else:
    print("‚ùå Google API Key not set")

# ========== TOOL DEFINITIONS ==========
# Example pricing dictionary for Intensity's services (in INR)
service_prices = {
    "cloud storage basic": "‚Çπ8,000/month",
    "cloud storage enterprise": "‚Çπ41,000/month",
    "cloud migration": "‚Çπ82,000/project",
    "cybersecurity audit": "‚Çπ66,000",
    "cybersecurity monitoring": "‚Çπ25,000/month",
    "penetration testing": "‚Çπ1,25,000",
    "ai chatbot integration": "‚Çπ58,000/project",
    "ai analytics": "‚Çπ33,000/month",
    "ai custom solution": "Starting at ‚Çπ2,50,000"
}

def get_service_price(service_name: str) -> str:
    """
    Lookup tool for Intensity's services pricing (in INR).
    Takes a service name, normalizes it, and returns the price if available.
    """
    print(f"üîß Tool get_service_price called for: {service_name}")
    service = service_name.lower().strip()
    return service_prices.get(
        service,
        "Price not available ‚Äì please contact an Intensity expert for details."
    )

# Define tool schema
get_service_price_tool = {
    "type": "function",
    "function": {
        "name": "get_service_price",
        "description": "Look up the pricing for Intensity's cloud, cybersecurity, and AI services (in INR).",
        "parameters": {
            "type": "object",
            "properties": {
                "service_name": {
                    "type": "string",
                    "description": "The name of the Intensity service. Example: 'cloud storage basic', 'cybersecurity audit', 'ai chatbot integration'."
                }
            },
            "required": ["service_name"]
        }
    }
}

tools = [get_service_price_tool]

# ========== DUAL CHAT FUNCTIONS ==========
def chat_with_memory(message, history):
    """
    Chat function using ConversationalRetrievalChain with built-in memory
    This maintains conversation context across messages
    """
    print(f"üí¨ User message (with memory): {message}")
    
    try:
        # Use the conversation chain with built-in memory
        result = conversation_chain.invoke({"question": message})
        response = result["answer"]
        
        # Add to Gradio history
        history.append((message, response))
        print(f"ü§ñ Assistant response (memory): {response[:100]}...")
        return history, ""  # Clear the input box
        
    except Exception as e:
        error_response = f"Sorry, I encountered an error: {str(e)}"
        print(f"‚ùå Chat with memory error: {e}")
        history.append((message, error_response))
        return history, ""

def chat_with_tools(message, history):
    """
    Chat function using direct OpenAI API with tool support
    This uses the vector store for RAG but doesn't maintain conversation memory
    """
    print(f"üí¨ User message (with tools): {message}")
    
    # STEP 1: Search vector database for relevant information (RAG)
    try:
        relevant_docs = vectorstore.similarity_search(message, k=3)
        context = "\n\n".join([f"From {doc.metadata.get('doc_type', 'unknown')}:\n{doc.page_content}" 
                              for doc in relevant_docs])
        print(f"üìö Found {len(relevant_docs)} relevant documents")
    except Exception as e:
        print(f"‚ùå RAG search failed: {e}")
        context = "No relevant documents found."
    
    # STEP 2: Build messages with context
    messages = [
        {
            "role": "system", 
            "content": f"{system_message}\n\nIMPORTANT: Use this context from company documents when relevant:\n{context}"
        }
    ]
    
    # Add conversation history from Gradio
    for entry in history:
        if isinstance(entry, tuple) and len(entry) == 2:
            user_msg, assistant_msg = entry
            messages.append({"role": "user", "content": user_msg})
            if assistant_msg:
                messages.append({"role": "assistant", "content": assistant_msg})
    
    # Add current message
    messages.append({"role": "user", "content": message})
    
    try:
        # STEP 3: Get response with tool support
        completion = openai_client.chat.completions.create(
            model=MODEL,
            messages=messages,
            tools=tools,
            tool_choice="auto"
        )

        response_message = completion.choices[0].message

        # STEP 4: Handle tool calls if needed
        if response_message.tool_calls:
            tool_call = response_message.tool_calls[0]
            function_name = tool_call.function.name
            arguments = json.loads(tool_call.function.arguments)

            if function_name == "get_service_price":
                price_result = get_service_price(arguments['service_name'])
                response = f"The price for **{arguments['service_name']}** is {price_result}."
                print(f"üí∞ Price lookup result: {response}")
            else:
                response = "I don't know how to use that tool yet."
        else:
            # Normal assistant response
            response = response_message.content

        # Append to history and return
        history.append((message, response))
        print(f"ü§ñ Assistant response (tools): {response[:100]}...")
        return history, ""  # Clear the input box
        
    except Exception as e:
        error_response = f"Sorry, I encountered an error: {str(e)}"
        print(f"‚ùå Chat with tools error: {e}")
        history.append((message, error_response))
        return history, ""

# ========== GRADIO UI WITH DUAL MODES ==========
print("üöÄ Building chat interface...")

# Test the conversational chain
print("üß™ Testing conversational chain...")
try:
    test_result = conversation_chain.invoke({"question": "Can you describe the company services?"})
    print(f"‚úÖ Test successful: {test_result['answer'][:100]}...")
except Exception as e:
    print(f"‚ùå Conversational chain test failed: {e}")

with gr.Blocks(theme=gr.themes.Soft()) as demo:
    gr.Markdown("""
    # ü§ñ Intensity AI Assistant
    **Your intelligent companion for company information and services**
    
    I can help you with:
    - üìÑ Answering questions from company documents (with memory)
    - üí∞ Providing service pricing information (with tools)
    - üîç Finding relevant information across all departments
    
    *Choose your chat mode below:*
    """)
    
    with gr.Row():
        with gr.Column(scale=1):
            chat_mode = gr.Radio(
                choices=["üß† Chat with Memory", "üîß Chat with Tools"],
                value="üß† Chat with Memory",
                label="Chat Mode",
                info="Memory mode maintains conversation context, Tools mode enables pricing lookup"
            )
        
        with gr.Column(scale=4):
            chatbot = gr.Chatbot(
                height=500,
                placeholder="Ask me anything about Intensity's services or company information...",
                show_copy_button=True
            )
    
    msg = gr.Textbox(
        placeholder="Type your question here...",
        label="Your Message",
        scale=7
    )
    
    with gr.Row():
        submit = gr.Button("üöÄ Send", variant="primary", scale=1)
        clear_memory = gr.Button("üßπ Clear Memory", scale=1)
        clear_chat = gr.Button("üóëÔ∏è Clear Chat", scale=1)

    def handle_chat(message, history, mode):
        """Route to appropriate chat function based on mode"""
        if mode == "üß† Chat with Memory":
            return chat_with_memory(message, history)
        else:  # "üîß Chat with Tools"
            return chat_with_tools(message, history)
    
    def clear_conversation_memory():
        """Clear the conversation chain memory"""
        global conversation_chain, memory
        memory.clear()
        # Recreate the chain with fresh memory
        conversation_chain = ConversationalRetrievalChain.from_llm(
            llm=llm, 
            retriever=retriever, 
            memory=memory,
            return_source_documents=True
        )
        print("‚úÖ Conversation memory cleared!")
        return None

    # Handle message submission
    submit.click(
        fn=handle_chat,
        inputs=[msg, chatbot, chat_mode],
        outputs=[chatbot, msg]
    )
    
    # Allow pressing Enter to send message
    msg.submit(
        fn=handle_chat,
        inputs=[msg, chatbot, chat_mode],
        outputs=[chatbot, msg]
    )
    
    # Clear conversation memory
    clear_memory.click(
        fn=clear_conversation_memory,
        outputs=chatbot
    )
    
    # Clear chat history only
    clear_chat.click(lambda: None, None, chatbot, queue=False)
    
    gr.Markdown("""
    ---
    *Powered by LangChain ‚Ä¢ Chroma ‚Ä¢ HuggingFace ‚Ä¢ OpenAI*
    
    **Mode Explanation:**
    - **Chat with Memory**: Maintains conversation context across messages, better for extended discussions
    - **Chat with Tools**: Enables pricing lookup functionality, better for specific service inquiries
    """)

# ========== APPLICATION LAUNCH ==========
if __name__ == "__main__":
    print("üéâ Starting Intensity AI Assistant...")
    print("üåê Opening web interface...")
    demo.launch(
        share=False, 
        debug=True,
        show_error=True
    )
